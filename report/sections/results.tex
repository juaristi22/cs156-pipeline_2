\section{Results and Discussion}
\label{sec:results}

% Addresses instructions.md sections 6-8

\subsection{Implementation Procedure}

To evaluate model performance before final imputation, we employ 3-fold cross-validation on the preprocessed SCF dataset. This approach assesses how well each model generalizes to unseen data by using the donor survey where ground-truth net worth values are available. Nonetheless, it is important to consider that the use of cross-validation loss as a metric strongly relies on assumption that the CPS population does not differ systematically from the SCF population in ways not captured by the shared covariates.

For the procedure, the SCF dataset is randomly partitioned into three subsets of approximately equal size. The number of folds was selected hoping to provide enough measure for variability in performance while keeping computational constraints in mind. For each fold, the model is trained on the union of the other two folds (approximately 15,300 observations). Then, predictions are generated for the held-out fold (approximately 7,650 observations), and quantile loss is computed at each quantile level by comparing predictions to true net worth values. For each quantile level $\tau$, we report the mean and standard deviation of quantile loss across the three folds. The standard deviation provides insight into the stability of each model's performance, with high variance indicating sensitivity to the particular training sample or overfitting.

After cross-validation, each model was trained on the full SCF dataset and used to impute net worth onto the CPS. Given the computational requirements of training each deep learning method, and the relative miniscule time required to impute once they are trained, the model saving mechanisms were implemented to avoid unnecessary computational costs. This enables reusing the same network weights for future imputations without retraining, as long as the donor dataset and preprocessing are consistent across runs. The imputed CPS distributions were also saved for subsequent analysis and benchmarking evaluation.

\subsection{Evaluation Metrics}
\label{sec:metrics}

Evaluating imputation quality to benchmark the performance of different models requires metrics that assess both pointwise accuracy and distributional fidelity. We employ three complementary metrics: quantile loss for cross-validation on the donor survey, and both Wasserstein distance and the Kolmogorov-Smirnov statistic for comparing the final imputed distribution against the target.

\subsubsection{Quantile Loss (Cross-Validation)}

Quantile loss, introduced by \citet{koenker1978regression}, measures how well a model's predictions capture specific quantiles of the conditional distribution. For a given quantile level $\tau \in (0,1)$, the loss is defined as:
\begin{equation}
\mathcal{L}_\tau(y, \hat{y}) = \frac{1}{N} \sum_{i=1}^{N} \rho_\tau(y_i - \hat{y}_i)
\end{equation}
where $\rho_\tau(u) = u(\tau - \mathbf{1}_{u < 0})$ is the pinball loss. This asymmetric loss penalizes over-predictions and under-predictions differently depending on $\tau$. For example, for a high quantile like $\tau = 0.9$, under-predictions are penalized 9 times more heavily than over-predictions.

We evaluate at different quantile levels ($\tau \in \{0.25, 0.5, 0.75\}$) to assess performance across the entire conditional distribution. A model that accurately captures the full distribution should achieve low quantile loss at all levels, not just the median.

\subsubsection{Wasserstein Distance}

While quantile loss evaluates conditional predictions, we also need to assess whether the marginal distribution of imputed values matches the target population. The 1-Wasserstein distance (also known as the Earth Mover's Distance) \citep{villani2008optimal} quantifies the minimum ``cost'' of transforming one distribution into another:
\begin{equation}
W_1(P, Q) = \inf_{\gamma \in \Gamma(P,Q)} \mathbb{E}_{(y, \hat{y}) \sim \gamma} [|y - \hat{y}|]
\end{equation}
where $\Gamma(P,Q)$ is the set of all joint distributions with marginals $P$ and $Q$.

For univariate distributions, the Wasserstein distance simplifies to the integral of the absolute difference between quantile functions:
\begin{equation}
W_1(P, Q) = \int_0^1 |F_P^{-1}(u) - F_Q^{-1}(u)| \, du
\end{equation}
This formulation accommodates survey weights by constructing weighted empirical quantile functions. A lower Wasserstein distance indicates that the imputed CPS distribution more closely matches the SCF wealth distribution, which is essential when wanting to use that data to produce reliable policy analysis and distributional impact assessments.

\subsubsection{Kolmogorov-Smirnov Statistic}

The Kolmogorov-Smirnov (KS) statistic \citep{kolmogorov1933sulla, smirnov1948table} provides a complementary measure of distributional similarity that focuses on the shape of distributions rather than absolute distances. The two-sample KS statistic is defined as the maximum absolute difference between the empirical cumulative distribution functions (CDFs) of two samples:
\begin{equation}
D_{n,m} = \sup_x |F_n(x) - G_m(x)|
\end{equation}
where $F_n$ and $G_m$ are the empirical CDFs of the donor (SCF) and imputed (CPS) distributions with sample sizes $n$ and $m$, respectively.

Unlike the Wasserstein distance, which measures the total ``work'' required to transform one distribution into another and is sensitive to the magnitude of differences in absolute terms, the KS statistic is bounded between 0 and 1 and captures the maximum pointwise discrepancy between CDFs. This makes the KS statistic particularly useful for detecting differences in distributional shape without being dominated by extreme values. For heavy-tailed distributions like wealth, where outliers can disproportionately influence the Wasserstein distance, the KS statistic offers a more balanced assessment of how well the overall distributional form is preserved.

\subsection{Cross-Validation Results}

Table~\ref{tab:cv_results} presents the 3-fold cross-validation results for all models. Results are reported as mean $\pm$ standard deviation across folds, with lower values indicating better performance.

\begin{table}[H]
\centering
\caption{3-Fold Cross-Validation Results on SCF Data (Quantile Loss)}
\label{tab:cv_results}
\begin{tabular}{lccc}
\toprule
Model & $\mathcal{L}_{0.25}$ & $\mathcal{L}_{0.50}$ & $\mathcal{L}_{0.75}$ \\
\midrule
QRF (baseline) & $2.51 \pm 0.03$ & $2.51 \pm 0.01$ & $2.46 \pm 0.05$ \\
MDN & $\mathbf{2.08 \pm 0.07}$ & $\mathbf{2.11 \pm 0.07}$ & $\mathbf{2.14 \pm 0.07}$ \\
RealNVP & $3.17 \pm 0.22$ & $3.36 \pm 0.17$ & $3.55 \pm 0.26$ \\
TabSyn & $3.49 \pm 0.02$ & $6.23 \pm 0.06$ & $8.98 \pm 0.10$ \\
\bottomrule
\end{tabular}
\end{table}

The Quantile Random Forest baseline achieves consistent performance across all quantile levels, with low variance across folds (standard deviations between 0.01 and 0.05). This stability reflects the robustness of tree-based ensemble methods to different training samples. The QRF's quantile loss values are moderate, providing a reasonable benchmark against which to evaluate the deep learning approaches.

However, it is the Mixture Density Network that achieves the best cross-validation performance, with losses approximately 15--20\% lower than the QRF baseline. The MDN's consistent performance across $\mathcal{L}_{0.25}$, $\mathcal{L}_{0.50}$, and $\mathcal{L}_{0.75}$ suggests that the Gaussian mixture parameterization effectively captures the conditional distribution of net worth given the predictor variables. The relatively low standard deviations indicate stable performance across folds.

RealNVP shows moderate cross-validation performance, with quantile losses higher than both QRF and MDN. The higher variance across folds (standard deviations of 0.17--0.26) suggests some sensitivity to the particular training sample. The normalizing flow architecture may require more data or hyperparameter tuning to achieve optimal performance on this task.

Meanwhile, TabSyn exhibits the worst performance, with loss values degrading substantially at higher quantiles. While $\mathcal{L}_{0.25}$ is comparable to RealNVP, $\mathcal{L}_{0.75}$ is nearly three times larger. This asymmetry suggests that the VAE-diffusion architecture consistently underpredicts, struggling to capture the upper tail of the wealth distribution during cross-validation. The low standard deviations indicate this pattern spans across folds rather than being a result of sampling variability.

\subsection{Final Imputation Results}

\begin{table}[H]
\centering
\caption{Distributional Accuracy: Imputed CPS vs. SCF Donor Distribution}
\label{tab:distributional}
\begin{tabular}{lcc}
\toprule
Model & Wasserstein Distance & KS Statistic \\
\midrule
QRF (baseline) & $\mathbf{471,900}$ & $\mathbf{0.041}$ \\
MDN & $1,199,237$ & $0.049$ \\
RealNVP & $13,809,212,015$ & $0.230$ \\
TabSyn & $12,665,210$ & $0.168$ \\
\bottomrule
\end{tabular}
\end{table}

The distributional accuracy results shed important light on each model's ability to preserve the overall wealth distribution in the imputed CPS data, complementing the cross-validation measure of conditional accuracy. Despite achieving the best CV quantile loss, MDN produces an imputed distribution with Wasserstein distance 2.5 times larger than QRF. Nonetheless, it is still the model with best distributional accuracy out of the deep learning methodologies studied. Contrastingly, RealNVP and TabSyn produce a Wasserstein distance four orders of magnitude larger than QRF, indicating very poor distributional preservation. The KS statistic provides a complementary perspective. QRF achieves the lowest KS statistic (0.041), indicating that its imputed CDF closely tracks the SCF distribution. MDN follows closely (0.049), while TabSyn (0.168) and RealNVP (0.230) show substantially larger maximum CDF deviations.

While QRF produces imputed values with a 99th percentile close to the SCF's \$13.2 million and a 1st percentile near the SCF's $-$\$72,000, accurately preserving both tails, and MDN closely approximates these percentiles with its Gaussian mixtures, the TabSyn model over-generates extreme values, with a 99th percentile of \$231 million (17 times too high). These implausible extremes dominate the Wasserstein distance calculation. RealNVP exhibits even more severe tail inflation with its wide close-to-symmetrical distribution, producing extreme values that result in the largest Wasserstein distance despite acceptable CV loss, and completely missing the true shape of the net worth distribution when imputing. It seems that certain models optimized for conditional prediction accuracy (minimizing expected loss) may learn to produce conservative, mean-regressing predictions that fail to preserve distributional properties, making it crucial to also measure distributional accuracy when benchmarking performance.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/model_comparison.png}
\caption{Log-transformed net worth weighted distributions comparing SCF donor data (grey) against imputed CPS values for each model. Dashed lines indicate median values. QRF and MDN closely match the SCF distribution shape and median, while RealNVP exhibits a shifted, symmetric distribution missing the characteristic right skew. TabSyn captures the general shape but over-generates extreme values in both tails.}
\label{fig:distributions}
\end{figure}

\subsection{Discussion}

Among the deep learning approaches evaluated, only the Mixture Density Network demonstrates performance comparable to the QRF baseline. MDN achieves the best cross-validation quantile loss and produces distributional accuracy metrics (Wasserstein distance and KS statistic) within the same order of magnitude as QRF. While MDN does not significantly outperform QRF on distributional metrics, its competitive performance suggests that neural network-based density estimation can be viable for survey imputation, potentially requiring additional work for hyperparameter tuning and more careful configuration.

In contrast, RealNVP and TabSyn seem to be unsuitable for this task with their current configurations, and working toward improving their architectures and training procedures may be unnecessarily complex for marginal improvement. RealNVP produces a nearly symmetric distribution centered far below the true median, completely failing to capture the characteristic right skew of the wealth distribution. TabSyn, despite its sophisticated VAE-diffusion architecture, generates implausible extreme values.

The relative success of MDN compared to the other deep learning methods may stem from its explicit probabilistic formulation. The Gaussian mixture parameterization directly models the conditional density $p(y|x)$, providing interpretable components that can capture multimodality and heteroskedasticity. Normalizing flows (RealNVP) and diffusion models (TabSyn), while theoretically more flexible, may require more careful tuning or architectural modifications to handle the specific challenges of wealth data.

\subsubsection{Computational Considerations}

The models differ substantially in computational requirements. QRF training completes in under 2 minutes on a standard CPU, leveraging the efficiency of tree-based methods. MDN requires approximately 15--20 minutes for 100 epochs of neural network training. RealNVP has similar computational costs to MDN for 500 epochs of flow training. Given the competitive performance of MDN, its moderate training time may be justifiable for researchers seeking a deep learning alternative when seeking flexibility and interpretability.

TabSyn is by far the most computationally intensive, requiring separate VAE pre-training (2,000 epochs) followed by diffusion model training (~2,000 epochs considering its early stopping). Total training time exceeds several hours even on GPU hardware. This computational burden, combined with the poor empirical performance observed, makes TabSyn impractical for survey imputation in its current form.

\subsubsection{Implications for Survey Data Fusion}

Our findings suggest that while most deep generative models tested here fail to outperform traditional methods, MDN represents a viable alternative worthy of further investigation. The disconnect between CV loss and distributional accuracy observed across models highlights that standard model selection criteria may be insufficient for imputation tasks where preserving marginal distributions is essential.

\subsubsection{Limitations and Future Work}

Several limitations should be considered when interpreting these results:

\begin{itemize}
    \item \textbf{Covariate overlap assumption}: Statistical matching assumes that $P(Y|X)$ is identical across surveys. Given that SCF and CPS populations differ in year (2022 vs 2023), imputation quality suffers regardless of model choice. Nonetheless, this limitation affects all models equally, so relative performance comparisons remain valid.
    \item \textbf{Limited predictor set}: We use only age, gender, race, and income variables as predictors. Wealth is influenced by many factors (education, occupation, inheritance, homeownership) not available in both surveys, limiting achievable imputation accuracy.
    \item \textbf{Sample size}: The SCF contains approximately 23,000 observations after preprocessing. While adequate for tree-based methods, deep learning approaches typically benefit from larger training sets.
    \item \textbf{Hyperparameter sensitivity}: The deep learning models' performance depends on architecture and training choices that were not exhaustively optimized in this study.
\end{itemize}

Given MDN's promising results, future work should explore whether alternative hyperparameter configurations could enable it to surpass QRF. Potential directions include increasing the number of Gaussian mixture components beyond the five used, as well as the training time to determine whether it yields improvement or results in overfitting. Incorporating regularization techniques specifically designed to prevent distribution compression could also help address some of the limitations observed in our results. Experimenting with alternative mixture distributions (e.g., Student-t components) that naturally accommodate heavier tails than Gaussians may be one of the more promising avenues to capture more complex distributions in other contexts, while deeper or wider architectures may help improve the network's capacity to learn complex conditional relationships.
