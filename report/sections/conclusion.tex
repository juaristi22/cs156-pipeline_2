\section{Conclusion}
\label{sec:conclusion}

\subsection{Summary}

This paper investigated whether deep generative models can improve upon Quantile Random Forests for the task of imputing net worth from the Survey of Consumer Finances onto the Current Population Survey. We evaluated three deep learning architectures---RealNVP (normalizing flows), Mixture Density Networks, and TabSyn (VAE with latent diffusion)---against a QRF baseline, using both cross-validation quantile loss and distributional accuracy metrics.

Despite being a traditional ensemble method, QRF remains the strongest overall performer, achieving the best distributional accuracy (Wasserstein distance of 472,000; KS statistic of 0.041) while maintaining competitive cross-validation performance. Among the deep learning approaches, only MDN demonstrates competitive performance. MDN achieves 16\% lower median quantile loss than QRF and produces distributional metrics within the same order of magnitude. Its explicit Gaussian mixture parameterization provides interpretable density estimation that generalizes reasonably well to the receiver dataset, suggesting that neural network-based methods can be viable for survey imputation with more fine-tuned architectural choices.

In contrast, RealNVP and TabSyn prove unsuitable, at least with their current configurations. Both models produce imputed distributions with Wasserstein distances exceeding 12 million, which are orders of magnitude worse than QRF. RealNVP generates a symmetric distribution missing the characteristic right skew of wealth, while TabSyn over-generates extreme values in both tails. These failures highlight the challenges of applying flexible generative models to heavy-tailed survey data without careful adaptation. The heavy computational burdern of TabSyn further limits its practicality for this task, while the reasonable training time of MDN makes it a more accessible deep learning alternative for researchers seeking a very flexible imputation approach. 

Overall, MDN may become a promising model as it seems to competitively capture complex distributions like wealth, while offering additional flexibility, which may be beneficial in other imputation contexts. Nonetheless, they remain a method with higher training data and computational demands compared to QRF, which continues to serve as a robust, efficient baseline for survey imputation tasks where preserving distributional characteristics.

\subsection{Pipeline Overview}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/pipeline.jpg}
\caption{Overview of the imputation and model benchmarking pipeline.}
\label{fig:pipeline}
\end{figure}

The imputation pipeline developed in this work consists of five stages. First, the SCF and CPS datasets are loaded and harmonized by filtering to common predictor variables: age, gender, race, and income sources (employment, interest/dividend, and pension income). Next, net worth values undergo inverse hyperbolic sine (asinh) transformation to handle the heavy-tailed distribution, while categorical variables are encoded appropriately for each model (one-hot encoding for neural networks, native categorical handling for QRF).

Model evaluation proceeds through three-fold cross-validation on the SCF, assessing each model's ability to predict held-out net worth values by computing quantile loss at $\tau \in \{0.25, 0.50, 0.75\}$. Following cross-validation, models are trained on the full SCF dataset and used to impute net worth onto all CPS observations, with stochastic sampling preserving distributional properties for generative models. Finally, imputed CPS distributions are compared against the SCF donor distribution using Wasserstein distance and the Kolmogorov-Smirnov statistic to assess distributional fidelity.

An additional step involved adapting TabSyn's architecture to condition its diffusion sampling on receiver observations, a necessary modification for the statistical matching context. This entailed altering the model's input structure and preprocessing procedure, as well as supporting conditioning on a dataset other than the one used to train through Symlinks to ensure that generated samples reflected the predictor values present in the receiver dataset.

\subsection{Reproducibility and Future Work}

All code for this analysis is available at \url{https://github.com/juaristi22/cs156-pipeline_2}, including TabSyn's adapted implementations and the Jupyter notebook with the full pipeline. The SCF data is publicly available from the Federal Reserve (\url{https://www.federalreserve.gov/econres/scfindex.htm}), and the CPS data can be accessed in PolicyEngine's public huggingface repository (\url{https://huggingface.co/policyengine/policyengine-us-data/blob/main/cps\_2023.h5}).

Future work should explore whether MDN can surpass QRF with alternative configurations, including more mixture components, heavier-tailed distributions (e.g., Student-t mixtures), and deeper network architectures. Additionally, investigating why normalizing flows and diffusion models struggle with this task could inform architectural modifications that better handle heavy-tailed survey data. Finally, extending the predictor set in the case that data collection and survey design is extended to additional variables may improve imputation accuracy across all methods.